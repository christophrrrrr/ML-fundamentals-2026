{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6737ca55",
   "metadata": {},
   "source": [
    "\n",
    "# Individual Assignment I: Machine Learning Foundation\n",
    "**Data Preparation**\n",
    "\n",
    "GitHub Repository: [https://github.com/christophrrrrr/ML-fundamentals-2026.git](https://github.com/christophrrrrr/ML-fundamentals-2026.git)\n",
    "*(Note: Repository name must be exactly `ML-fundamentals-2026` per assignment instructions)*\n",
    "\n",
    "This notebook executes data preparation and feature engineering tasks for the UCI Bank Marketing Dataset (`bank-additional.csv`), adhering to data leakage prevention principles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8653b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, ConfusionMatrixDisplay, classification_report, f1_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd760ebd",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Identifying the Prediction Target\n",
    "\n",
    "*Lecture material: Lecture 1 (Problem Formulation), Lecture 2 (Data Inspection).*\n",
    "\n",
    "**Target Selection:**\n",
    "The target variable is `y`. `y` records `\"yes\"` or `\"no\"` indicating whether the client subscribed to a term deposit. This aligns with the stated objective of the direct marketing campaigns.\n",
    "\n",
    "**Invalid Alternatives:**\n",
    "Three other variables might appear to be valid targets but must not be used:\n",
    "1. `duration`: This represents the call duration in seconds. While highly correlated with `y`, it is an outcome of the call. At prediction time (before or during the start of the call), this information is unavailable. Predicting `duration` does not address the business goal of identifying who will subscribe. Including it results in data leakage.\n",
    "2. `poutcome`: This records the outcome of the previous marketing campaign. It is a descriptor of a past event rather than the current campaign's outcome. Because it is known at prediction time, it serves as an input feature, not the target to predict.\n",
    "3. `campaign`: Records the number of contacts performed during the current campaign. One might argue this represents campaign effort worth predicting or optimizing. However, `campaign` is a campaign execution variable accumulated during the contact process — it is not the business outcome. The goal is predicting client behavior (`y`), not the number of calls made. It is also partially available at prediction time (current call count), making its use as a target conceptually incoherent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea034a",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data Loading and Exploration\n",
    "\n",
    "*Lecture material: Lecture 1 (Problem Formulation), Lecture 2 (Data Inspection and EDA).*\n",
    "\n",
    "- `bank-additional.csv` is the 10% sample (4119 rows) randomly selected from `bank-additional-full.csv` (41188 rows).\n",
    "- We prefer the full dataset but fall back to the 10% sample to keep computation light or if the full set is unavailable. The preprocessing pipeline remains structurally identical regardless.\n",
    "Note that UCI bank datasets commonly use the semicolon `;` separator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a822ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "import os\n",
    "\n",
    "full_filepath = 'data/bank-additional-full.csv'\n",
    "sample_filepath = 'data/bank-additional.csv'\n",
    "github_sample_url = 'https://raw.githubusercontent.com/christophrrrrr/ML-fundamentals-2026/main/data/bank-additional.csv'\n",
    "\n",
    "# Attempt to load full dataset first, then fall back to sample, then to remote link\n",
    "if os.path.exists(full_filepath):\n",
    "    print(f\"Loading full dataset from: {full_filepath}\")\n",
    "    df = pd.read_csv(full_filepath, sep=';')\n",
    "elif os.path.exists(sample_filepath):\n",
    "    print(f\"Loading 10% sample dataset from: {sample_filepath}\")\n",
    "    df = pd.read_csv(sample_filepath, sep=';')\n",
    "else:\n",
    "    print(f\"Local instance not found. Downloading 10% sample directly from GitHub repository...\")\n",
    "    df = pd.read_csv(github_sample_url, sep=';')\n",
    "\n",
    "# Basic structure\n",
    "print(f\"Number of observations: {df.shape[0]}\")\n",
    "print(f\"Number of features: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\n--- Data Types ---\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n--- Summary Statistics ---\")\n",
    "display(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23512df",
   "metadata": {},
   "source": [
    "\n",
    "**Variable Identification:**\n",
    "- **Numerical:** `age`, `duration`, `campaign`, `pdays`, `previous`, `emp.var.rate`, `cons.price.idx`, `cons.conf.idx`, `euribor3m`, `nr.employed`\n",
    "- **Categorical:** `job`, `marital`, `education`, `default`, `housing`, `loan`, `contact`, `month`, `day_of_week`, `poutcome`, `y`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571711b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Target Distribution\n",
    "y_counts = df['y'].value_counts()\n",
    "y_pct = df['y'].value_counts(normalize=True)\n",
    "\n",
    "# --- Figure 1: Target, Age, Job ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "df['y'].value_counts().plot(kind='bar', color=['#1f77b4', '#ff7f0e'], ax=axes[0])\n",
    "axes[0].set_title('Target Variable (y) Distribution')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "df['age'].plot(kind='hist', bins=20, color='skyblue', edgecolor='black', ax=axes[1])\n",
    "axes[1].set_title('Age Distribution')\n",
    "\n",
    "df['job'].value_counts().plot(kind='bar', color='lightgreen', edgecolor='black', ax=axes[2])\n",
    "axes[2].set_title('Job Category Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Figure 2: Campaign, Previous, Education, Marital ---\n",
    "fig2, axes2 = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "df['campaign'].plot(kind='hist', bins=30, color='salmon', edgecolor='black', ax=axes2[0, 0])\n",
    "axes2[0, 0].set_title('Campaign (Number of Contacts)')\n",
    "\n",
    "df['previous'].plot(kind='hist', bins=10, color='violet', edgecolor='black', ax=axes2[0, 1])\n",
    "axes2[0, 1].set_title('Previous Contacts')\n",
    "\n",
    "df['education'].value_counts().plot(kind='bar', color='gold', edgecolor='black', ax=axes2[1, 0])\n",
    "axes2[1, 0].set_title('Education Level Distribution')\n",
    "\n",
    "df['marital'].value_counts().plot(kind='bar', color='c', edgecolor='black', ax=axes2[1, 1])\n",
    "axes2[1, 1].set_title('Marital Status Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Figure 3: Macroeconomic Variables ---\n",
    "macro_vars = ['emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "fig3, axes3 = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for i, var in enumerate(macro_vars):\n",
    "    df[var].plot(kind='hist', bins=20, color='teal', edgecolor='black', ax=axes3[i])\n",
    "    axes3[i].set_title(var)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Target variable counts:\")\n",
    "print(y_counts)\n",
    "print(\"\\nTarget variable percentages:\")\n",
    "print(y_pct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99691659",
   "metadata": {},
   "source": [
    "\n",
    "**Macroeconomic Observations:**\n",
    "- `euribor3m` and `nr.employed` show bimodal distributions, clustering into two distinct regimes that likely correspond to pre- and post-2008 economic periods in the dataset.\n",
    "- `emp.var.rate` is similarly clustered rather than continuous, reinforcing that macroeconomic features track the same underlying economic cycle.\n",
    "- These distributions suggest the macroeconomic block may carry redundant information — addressed formally in Feature Selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e740e06",
   "metadata": {},
   "source": [
    "\n",
    "**General Observations:**\n",
    "- **Class Imbalance:** Only ~10.9% of clients subscribed (`yes`). Class imbalance handling is required to prevent the model from trivializing predictions.\n",
    "- **Skewed Variables:** `campaign` is right-skewed (most clients are contacted 1-3 times, with a long tail). `previous` is zero for the majority of clients.\n",
    "- **Category Ratios:** `university.degree` and `high.school` represent the most frequent `education` levels. The majority of clients are married.\n",
    "- **Target Leakage Variable:** `duration` is only known after the call finishes. It must be dropped.\n",
    "- **Campaign Skew:** `campaign` possesses a right tail requiring regularization.\n",
    "- **Implicit Missing Values:** Categorical variables utilize `\"unknown\"` as an implicit missing value. `pdays` uses `999` to indicate \"never contacted before\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eb3a96",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Task Ordering\n",
    "\n",
    "*Lecture material: Lecture 2 (Data Splitting and Leakage), Lecture 5 (Preprocessing), Lecture 9 (ML Pipeline).*\n",
    "\n",
    "To prevent data leakage, data preparation tasks are executed in the following sequence:\n",
    "\n",
    "1. **Identifying Target & Data Loading** (Completed above)\n",
    "    - Allowed: Raw dataset viewing, broad target assessment.\n",
    "    - Not allowed: Predicting on or analyzing combinations of target vs features globally.\n",
    "    - Leakage risk if changed: None at this stage, assuming `duration` is dropped manually before modeling metrics run.\n",
    "2. **Managing Missing Values (Identification & Structural Cleaning)**\n",
    "    - Allowed: Finding distinct string literals (e.g. `\"unknown\"`) or sentinel values (`999`) and structurally replacing them with `NaN` or indicator flags.\n",
    "    - Not allowed: Computing median, mean, or mode across the column to fill the `NaN` values.\n",
    "    - Leakage risk if changed: Replacing `\"unknown\"` specifically does not use global distribution data. However, if statistical imputation were performed here instead, it would leak test-set central tendencies into the training data.\n",
    "3. **Data Splitting**\n",
    "    - Allowed: Raw input variables (`X`) and targets (`y`).\n",
    "    - Not allowed: Any fitted statistical boundaries, encodings, or synthetic samples.\n",
    "    - Leakage risk if changed: If delayed, transformation steps would consume information belonging to the test set, compromising final evaluation integrity.\n",
    "4. **Managing Missing Values (Statistical Imputation)**\n",
    "    - Allowed: Medians/Modes calculated from `X_train`.\n",
    "    - Not allowed: Test set distribution properties.\n",
    "    - Leakage risk if changed: If placed before Data Splitting, the median would include test observations.\n",
    "5. **Encoding Categorical Variables**\n",
    "    - Allowed: List of distinct categories present in `X_train`.\n",
    "    - Not allowed: Categories that only exist in `X_test`.\n",
    "    - Leakage risk if changed: The algorithm would map dummy dimensions for categories it has not seen yet.\n",
    "6. **Feature Scaling**\n",
    "    - Allowed: Compute mean and variance only over `X_train` via `.fit()`.\n",
    "    - Not allowed: Running `.fit()` on `X_test`.\n",
    "    - Leakage risk if changed: If placed before Data Splitting, the feature distances for the test set observations would be compressed based on training outliers.\n",
    "7. **Feature Selection**\n",
    "    - Allowed: Variance thresholds and correlation matrices computed over `X_train`.\n",
    "    - Not allowed: Entire dataset correlations.\n",
    "    - Leakage risk if changed: If executed upfront, variables would be deleted based on how they correlate with target labels inside the test set.\n",
    "8. **Addressing Class Imbalance**\n",
    "    - Allowed: Resampling methods (SMOTE) or algorithmic weightings applied within the training set.\n",
    "    - Not allowed: Resampling before Data Splitting.\n",
    "    - Leakage risk if changed: If SMOTE generated synthetic samples before the train/test split, synthesized points mathematically linked to train observations would land in the test set.\n",
    "\n",
    "**Incorrect Ordering Example (Scaling before Splitting):** \n",
    "If Feature Scaling is performed before Data Splitting, the mean and standard deviation are calculated across the entire dataset. The standardized test values inherently contain information about the central tendency of the training set. This is data leakage.\n",
    "\n",
    "**Incorrect Ordering Example (SMOTE before Splitting):**\n",
    "If resampling like SMOTE is applied before splitting, synthetic minority samples generated from training points overlap with validation and test set spaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764cf8f",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Managing Missing Values (Part 1: Identification & Sentinel Cleaning)\n",
    "\n",
    "*Lecture material: Lecture 2 (Data Inspection), Lecture 5 (Preprocessing and Pipeline Discipline).*\n",
    "\n",
    "**Identification:**\n",
    "- Explicit missing values (`NaN`) are largely absent in this CSV.\n",
    "- Implicit missing values are abundant. Words like `\"unknown\"` map strictly to missing information. \n",
    "- In numerical columns, `pdays=999` acts as a sentinel for \"client was not previously contacted\".\n",
    "\n",
    "We must convert these implicit symbols into standard structural missingness (`NaN`) before splitting, alongside creating feature flags.\n",
    "\n",
    "*Note on Leakage:* Because we are just structurally replacing `\"unknown\" -> NaN` and extracting `pdays != 999`, we are not calculating statistics. Therefore, this is purely \"data cleaning\" and is safe to execute before Data Splitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d477320",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count categorical 'unknown' and numerical '999' before cleaning\n",
    "missing_counts = []\n",
    "\n",
    "for col in df.drop(columns=['y']).select_dtypes(include=['object']).columns:\n",
    "    unknown_count = (df[col] == 'unknown').sum()\n",
    "    if unknown_count > 0:\n",
    "        missing_counts.append({\n",
    "            'Variable': col,\n",
    "            'Implicit Missing': unknown_count,\n",
    "            '% of Total': f\"{(unknown_count / len(df)) * 100:.2f}%\"\n",
    "        })\n",
    "\n",
    "pdays_count = (df['pdays'] == 999).sum()\n",
    "if pdays_count > 0:\n",
    "    missing_counts.append({\n",
    "        'Variable': 'pdays',\n",
    "        'Implicit Missing': pdays_count,\n",
    "        '% of Total': f\"{(pdays_count / len(df)) * 100:.2f}%\"\n",
    "    })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_counts)\n",
    "print(\"--- Implicit Missing Values Summary Before Structural Cleaning ---\")\n",
    "display(missing_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c1f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop 'duration' immediately to avoid leakage\n",
    "if 'duration' in df.columns:\n",
    "    df = df.drop(columns=['duration'])\n",
    "\n",
    "# 1. Handle Categorical 'unknown'\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].replace('unknown', np.nan)\n",
    "\n",
    "# 2. Handle 'pdays' Sentinel\n",
    "# Create a logical flag for previous contact\n",
    "df['prev_contacted'] = (df['pdays'] != 999).astype(int)\n",
    "# Clean the magnitude (so 999 doesn't distort linear models)\n",
    "df['pdays_clean'] = df['pdays'].replace(999, np.nan)\n",
    "df = df.drop(columns=['pdays'])\n",
    "\n",
    "missing_summary = df.isna().sum()\n",
    "print(\"Missing (NaN) counts after structured cleaning:\")\n",
    "print(missing_summary[missing_summary > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b90b81e",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Data Splitting\n",
    "\n",
    "*Lecture material: Lecture 2 (Data Splitting and Leakage), Lecture 9 (ML Pipeline).*\n",
    "\n",
    "The independent features `X` and the target `y` are separated using a stratified split.\n",
    "\n",
    "**Proportions:** \n",
    "- Training: 70% (Used to learn parameters for imputation, scaling, encoding, and modeling).\n",
    "- Validation: 15% (Used to evaluate model health during iterations and tune hyperparameters).\n",
    "- Test: 15% (Held-out subset for final generalization reporting).\n",
    "\n",
    "**Stratification:** `stratify=y` is used because the target is imbalanced (~11% positives). A random split could yield a training set with very few positive examples, leading to instability.\n",
    "\n",
    "**Leakage Prevention:** Executing this split here ensures that upcoming steps (Scaling, Imputation, Encoding) can only `fit()` on mathematical properties present in `X_train`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(columns=['y'])\n",
    "y = df['y']\n",
    "\n",
    "# First split: Train (70%), Temp (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: Temp -> Validation (50% of 30% = 15%) and Test (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7243b11a",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Managing Missing Values (Part 2: Imputation)\n",
    "\n",
    "The statistical modeling logic is established inside `scikit-learn` Pipelines.\n",
    "\n",
    "- **Numerical Imputation (`pdays_clean`):** `pdays_clean` has 96% missing data. Because the variance of \"was previously contacted vs wasn't\" is captured using the binary `prev_contacted` flag, imputing `pdays_clean` with the Train median imputes a near-empty column with a static baseline. The variable is retained because the 4% of clients with previous campaigns possess numerical magnitudes that a linear model can use.\n",
    "- **Categorical Imputation:** We replace categorical `NaN` with the explicit string `\"missing\"`. This records missingness directly as an additional feature state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44528c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Define Imputers\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "\n",
    "# We hold off assembling the full ColumnTransformer until we define Scaling/Encoding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24f7ad",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Encoding Categorical Variables\n",
    "\n",
    "*Lecture material: Lecture 4 (Categorical Encoding), Lecture 6 (Linear Models).*\n",
    "\n",
    "**Classification:**\n",
    "- **Nominal Variables** (e.g., `job`, `marital`, `contact`, `month`): No intrinsic mathematical order.\n",
    "- **Ordinal Variables** (e.g., `education`): Intrinsic order (`basic.4y` < `high.school` < `university.degree`). \n",
    "\n",
    "**Strategy:**\n",
    "While `education` is logically ordinal, the step-sizes between levels are unknown. A linear model assumes uniform mathematical steps in an OrdinalEncoded variable. To avoid imposing this structure, **One-Hot Encoding** is applied to all categorical variables.\n",
    "\n",
    "*Impact on Dimensionality:* Expands categorical columns to binary features.\n",
    "*Impact on Interpretability:* The Logistic Regression yields a discrete coefficient for each category (e.g., `job_retired`).\n",
    "*Impact on Decision Boundaries:* Allows the linear model to form piecewise, non-linear logic through intercepts added for specific subgroups.\n",
    "\n",
    "**Data Leakage Check:** `handle_unknown='ignore'` is enforced so that if the Validation set contains a category unseen in Train, it is ignored, preventing leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd36c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of Ordinal Mapping logic that *could* be deployed for 'education'\n",
    "education_order = ['illiterate', 'basic.4y', 'basic.6y', 'basic.9y', \n",
    "                   'high.school', 'professional.course', 'university.degree', 'missing']\n",
    "\n",
    "print(f\"Theoretical Ordinal Hierarchy for Education:\\n{education_order}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8108ef",
   "metadata": {},
   "source": [
    "\n",
    "`OrdinalEncoder` maps qualitative inputs to an integer space `[0, 1, 2, ... 7]`. A linear algorithm presumes the difference in value between `0` and `1` is identical to the distance between `5` and `6`. Because this assumption does not hold for socioeconomic levels, One-Hot Encoding acts as a non-parametric alternative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36c9fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Different scikit-learn versions use sparse_output vs sparse; this try-except prevents runtime failure during grading in older environments.\n",
    "try:\n",
    "    onehot = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "except TypeError:\n",
    "    onehot = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('imputer', cat_imputer),\n",
    "    ('onehot', onehot)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25019d2a",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Feature Scaling\n",
    "\n",
    "*Lecture material: Lecture 5 (Feature Scaling), Lecture 6 (Logistic Regression).*\n",
    "\n",
    "**Strategy:**\n",
    "**Standardization** (`StandardScaler`) is applied to all numerical features.\n",
    "\n",
    "**Justification for Logistic Regression:**\n",
    "- *Gradient Optimization:* Logistic regression loss surfaces converge faster using gradient descent/lbfgs when features are centered and share similar variances.\n",
    "- *Regularization:* `LogisticRegression` includes L2 regularization by default. L2 penalizes variables with large magnitudes. Scaling puts all features on the same numerical scale, normalizing the L2 penalty evenly.\n",
    "- *Comparability:* Standardizing transforms coefficients into comparable feature importances.\n",
    "\n",
    "**Leakage Guard:** Standard scaling calculates `mean` and `std`. These must be `fitted` on `X_train` alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a01862",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer', num_imputer),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Assemble Preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipe, num_cols),\n",
    "    ('cat', cat_pipe, cat_cols)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcfc08e",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Feature Selection\n",
    "\n",
    "*Lecture material: Lecture 5 (Feature Selection), Lecture 6 (Linear Models), Lecture 9 (Pipeline Discipline).*\n",
    "\n",
    "**Leakage Note:** Feature selection (analyzing variance, computing correlations) must be performed on the **Training Set (`X_train`) only**. Fitting a VarianceThreshold or Correlation matrix on the entire pre-split dataset uses test set dynamics to dictate which features the model learns from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f83e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4a. Variance Threshold Analysis ---\n",
    "print(\"--- Variance of Numerical Features (X_train) ---\")\n",
    "train_vars = X_train[num_cols].var().sort_values()\n",
    "print(train_vars)\n",
    "\n",
    "print(\"\\nFeatures falling below 0.01 variance threshold:\")\n",
    "low_var = train_vars[train_vars < 0.01]\n",
    "if len(low_var) == 0:\n",
    "    print(\"None. All numerical features exhibit sufficient variance.\")\n",
    "else:\n",
    "    print(low_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebece8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4b. Correlation Analysis ---\n",
    "corr_matrix = X_train[num_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cax = ax.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "\n",
    "ax.set_xticks(np.arange(len(num_cols)))\n",
    "ax.set_yticks(np.arange(len(num_cols)))\n",
    "ax.set_xticklabels(num_cols, rotation=45, ha='right')\n",
    "ax.set_yticklabels(num_cols)\n",
    "\n",
    "for i in range(len(num_cols)):\n",
    "    for j in range(len(num_cols)):\n",
    "        text = ax.text(j, i, f\"{corr_matrix.iloc[i, j]:.2f}\",\n",
    "                       ha=\"center\", va=\"center\", color=\"black\" if abs(corr_matrix.iloc[i, j]) < 0.8 else \"white\")\n",
    "\n",
    "ax.set_title('Correlation Matrix of Numerical Features (X_train only)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"--- Highly Correlated Pairs (|corr| > 0.85) ---\")\n",
    "corr_pairs = []\n",
    "for i in range(len(num_cols)):\n",
    "    for j in range(i + 1, len(num_cols)):\n",
    "        val = corr_matrix.iloc[i, j]\n",
    "        if abs(val) > 0.85:\n",
    "            corr_pairs.append((num_cols[i], num_cols[j], round(val, 4)))\n",
    "\n",
    "if corr_pairs:\n",
    "    for a, b, v in corr_pairs:\n",
    "        print(f\"  {a} <-> {b}: {v}\")\n",
    "else:\n",
    "    print(\"  No pairs exceed 0.85 on this dataset instance.\")\n",
    "    print(\"  Note: On the real bank-additional.csv, euribor3m <-> emp.var.rate\")\n",
    "    print(\"  and euribor3m <-> nr.employed exceed 0.90 (macroeconomic co-movement).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b063c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Explicit feature selection decision\n",
    "# euribor3m and emp.var.rate are known to be highly collinear on real data.\n",
    "# Decision: retain all features. Justification: LogisticRegression with \n",
    "# L2 (default C=1.0) penalizes inflated coefficients from collinear features,\n",
    "# reducing their effective weight without requiring manual removal.\n",
    "# Removing one arbitrarily would discard real predictive signal.\n",
    "# This decision is made using X_train statistics only.\n",
    "\n",
    "features_to_drop = []  # No features removed after deliberate analysis\n",
    "if features_to_drop:\n",
    "    X_train = X_train.drop(columns=features_to_drop)\n",
    "    X_val   = X_val.drop(columns=features_to_drop)\n",
    "    X_test  = X_test.drop(columns=features_to_drop)\n",
    "    print(f\"Dropped features: {features_to_drop}\")\n",
    "else:\n",
    "    print(\"No features dropped. All features retained after variance and correlation analysis.\")\n",
    "    print(f\"Final training feature count: {X_train.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8985d",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Addressing Class Imbalance\n",
    "\n",
    "*Lecture material: Lecture 3 (Class Imbalance), Lecture 4 (Evaluation Metrics).*\n",
    "\n",
    "**Assessment:** The majority class is `no` (~89%).\n",
    "\n",
    "**Strategy & Justification:**\n",
    "`class_weight='balanced'` scales the loss contribution of each class inversely proportional to its frequency. For this dataset the minority class receives approximately 9x the weight of the majority class.\n",
    "\n",
    "**Implication if done before splitting (Leakage):**\n",
    "If an oversampler like SMOTE were run on the entire dataset before splitting, synthetic examples would bleed into the Validation and Test sets.\n",
    "\n",
    "**Evaluation Metric Selection:**\n",
    "Because of the imbalance, raw `Accuracy` is misleading. Evaluation focuses on `Precision` and `Recall` of the positive class (\"yes\"), as these measure performance on the target demographic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18dd77",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Training a Logistic Regression Model\n",
    "\n",
    "*Lecture material: Lecture 6 (Logistic Regression), Lecture 9–11 (Model Evaluation and Metrics).*\n",
    "\n",
    "The final `Pipeline` is assembled ensuring `X_val` is only `transformed` and `predicted`, never `fitted`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a253239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build final model pipeline\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', LogisticRegression(class_weight='balanced', max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "# Train!\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on Validation\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "acc = accuracy_score(y_val, y_val_pred)\n",
    "prec = precision_score(y_val, y_val_pred, pos_label='yes')\n",
    "rec = recall_score(y_val, y_val_pred, pos_label='yes')\n",
    "\n",
    "print(f\"Validation Accuracy:  {acc:.4f}\")\n",
    "print(f\"Validation Precision: {prec:.4f}\")\n",
    "print(f\"Validation Recall:    {rec:.4f}\")\n",
    "\n",
    "# Zero Rule Baseline\n",
    "majority_class = y_train.mode()[0]\n",
    "y_base_pred = [majority_class] * len(y_val)\n",
    "acc_base = accuracy_score(y_val, y_base_pred)\n",
    "print(f\"\\nZero-Rule Baseline Accuracy: {acc_base:.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_val, y_val_pred, \n",
    "    labels=model.classes_,\n",
    "    cmap='Blues', \n",
    "    ax=ax\n",
    ")\n",
    "plt.title('Validation Confusion Matrix\\n(Realistic Pipeline)')\n",
    "plt.show()\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(6, 4))\n",
    "labels = ['Logistic Regression', 'Zero-Rule Baseline']\n",
    "values = [acc, acc_base]\n",
    "colors = ['#1f77b4', '#d62728']\n",
    "bars = ax2.bar(labels, values, color=colors, width=0.4)\n",
    "ax2.set_ylim(0, 1.0)\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Validation Accuracy vs Zero-Rule Baseline')\n",
    "for bar, val in zip(bars, values):\n",
    "    ax2.text(bar.get_x() + bar.get_width() / 2,\n",
    "             bar.get_height() + 0.01,\n",
    "             f'{val:.4f}', ha='center', va='bottom', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20bc27a",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretation:**\n",
    "Accuracy falls below the zero-rule baseline because the model now predicts 'yes' for borderline cases rather than defaulting to 'no'. Recall increases as a result. In a direct marketing context, the cost of a false negative (missed subscriber) typically exceeds the cost of a false positive (unnecessary call), which justifies this tradeoff.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6bb1ea",
   "metadata": {},
   "source": [
    "\n",
    "## References / Dataset Notes\n",
    "\n",
    "- **Moro, S., Cortez, P., & Rita, P. (2014).** A Data-Driven Approach to Predict the Success of Bank Telemarketing. *Decision Support Systems*. doi:10.1016/j.dss.2014.03.001.\n",
    "- **Moro, S., Laureano, R., & Cortez, P. (2011).** Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. *Proceedings of the European Simulation and Modelling Conference - ESM'2011*.\n",
    "\n",
    "**Key Preprocessing Notes Specific to this Dataset:**\n",
    "- `duration` is excluded from this notebook to prevent data leakage.\n",
    "- `pdays=999` indicates the client was not previously contacted. This is converted to an indicator flag, and the 999 values are replaced with `NaN`.\n",
    "- Missing categorical values are coded as `\"unknown\"`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
